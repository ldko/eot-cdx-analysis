{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting incoming CDX files to Parquet\n",
    "\n",
    "Quick look at file sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 9.5G May 18 17:56 eot2012_surt_index.cdx.gz\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   60 May 18 17:50 eot2012_surt_index.cdx.gz.md5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh eot2012_surt_index.cdx*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Spark can typically load `*.gz` files just fine, but that support comes from Hive integration, which seems to be missing here.  So gunzip first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gunzip eot2012_surt_index.cdx.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the unzipped file, filetering out any line that starts with a blank or has essentially no content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eot2012 = sc.textFile(\"eot2012_surt_index.cdx\") \\\n",
    "  .filter(lambda line: line[0] != ' ') \\\n",
    "  .filter(lambda line: len(line)>1) \\\n",
    "  .map(lambda line: line.split(\" \")) \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep a dataframe from the RDD, naming columns appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(eot2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_1\", \"surt_uri\") \\\n",
    "    .withColumnRenamed(\"_2\", \"capture_time\") \\\n",
    "    .withColumnRenamed(\"_3\", \"original_uri\") \\\n",
    "    .withColumnRenamed(\"_4\", \"mime_type\") \\\n",
    "    .withColumnRenamed(\"_5\", \"response_code\") \\\n",
    "    .withColumnRenamed(\"_6\", \"hash_sha1\") \\\n",
    "    .withColumnRenamed(\"_7\", \"redirect_url\") \\\n",
    "    .withColumnRenamed(\"_8\", \"meta_tags\") \\\n",
    "    .withColumnRenamed(\"_9\", \"length_compressed\") \\\n",
    "    .withColumnRenamed(\"_10\", \"warc_offset\") \\\n",
    "    .withColumnRenamed(\"_11\", \"warc_name\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"eot2012.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1G\teot2012.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs eot2012.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This final step took 15 minutes on an r3.4xlarge, using 3.9 hours of compute time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
